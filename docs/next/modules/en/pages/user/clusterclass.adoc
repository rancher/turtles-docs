= ClusterClass

In this section we cover using https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/[ClusterClass] with {product_name}.

== Prerequisites

* Rancher Manager cluster with {product_name} installed

== Setup

[tabs]
======

Azure::
+
To prepare the management Cluster, we are going to install the https://capz.sigs.k8s.io/[Cluster API Provider Azure], and create a https://capz.sigs.k8s.io/topics/identities#service-principal[ServicePrincipal] identity to provision a new Cluster on Azure. +
Before we start, a ServicePrincipal needs to be created, with at least Contributor access to an Azure subscription. +
Refer to the https://capz.sigs.k8s.io/getting-started#setting-up-your-azure-environment[CAPZ documentation] for more details. +
+
* Provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capz-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: azure
  namespace: capz-system
spec:
  type: infrastructure
  name: azure
----
+
* Identity setup
+
A Secret containing the ADD Service Principal password need to be created first.  
+
[source,bash]
----
# Settings needed for AzureClusterIdentity used by the AzureCluster
export AZURE_CLUSTER_IDENTITY_SECRET_NAME="cluster-identity-secret"
export AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE="default"
export AZURE_CLIENT_SECRET="<Password>"

# Create a secret to include the password of the Service Principal identity created in Azure
# This secret will be referenced by the AzureClusterIdentity used by the AzureCluster
kubectl create secret generic "${AZURE_CLUSTER_IDENTITY_SECRET_NAME}" --from-literal=clientSecret="${AZURE_CLIENT_SECRET}" --namespace "${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}"
----
+
The AzureClusterIdentity can now be created to use the Service Principal identity. +
Note that the AzureClusterIdentity is a namespaced resource and it needs to be created in the same namespace as the Cluster. +
For more information on best practices when using Azure identities, please refer to the official https://capz.sigs.k8s.io/topics/identities-use-cases[documentation]. +
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureClusterIdentity
metadata:
  labels:
    clusterctl.cluster.x-k8s.io/move-hierarchy: "true"
  name: cluster-identity
spec:
  allowedNamespaces: {}
  clientID: <AZURE_APP_ID>
  clientSecret:
    name: <AZURE_CLUSTER_IDENTITY_SECRET_NAME>
    namespace: <AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE>
  tenantID: <AZURE_TENANT_ID>
  type: ServicePrincipal
----

Docker::
+
To prepare the management Cluster, we are going to install the Docker Cluster API Provider.
+
* Infrastructure Docker provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capd-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: docker
  namespace: capd-system
spec:
  type: infrastructure
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----

vSphere::
+
To prepare Rancher management Cluster, we are going to install the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere[vSphere Cluster API Provider]. +
It is an example of vSphere provider installation, follow the provider documentation if some options need to be customized. +
+
* Infrastructure vSphere provider installation
+
[source, yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  name: capv-system
---
apiVersion: v1
kind: Secret
metadata:
  name: vsphere
  namespace: capv-system
type: Opaque
stringData:
  VSPHERE_USERNAME: xxx
  VSPHERE_PASSWORD: xxx
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: vsphere
  namespace: capv-system
spec:
  type: infrastructure
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----
+
* Credentials setup
+
A Secret containing the vSphere vCenter user and password need to be created first.
+
[source,bash]
----
# Settings needed for vSphere vCenter
export VCENTER_SECRET_NAME="vsphere-cluster-secret"
export VCENTER_USERNAME="<vCenter_UserName>"
export VCENTER_PASSWORD="<vCenter_Password>"

# Create a secret to include the username and password for vSpehere vCenter
kubectl create secret generic "${VCENTER_SECRET_NAME}" --from-literal=clientSecret="${VCENTER_USERNAME}" --namespace "${VCENTER_PASSWORD}"
----
+
* Cloud Provider Integration setup
+
To setup Cloud Provider Integration create following object resources *vsphere-config-secret*, *cloud-provider-vsphere-credentials* and *cpi-configmap* +
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: vsphere-config-secret
stringData:
  data: |-
    apiVersion: v1
    kind: Secret
    metadata:
      name: vsphere-config-secret
      namespace: vmware-system-csi
    stringData:
      csi-vsphere.conf: |+
        [Global]
        thumbprint = "${VSPHERE_TLS_THUMBPRINT}"

        [VirtualCenter "${VSPHERE_SERVER}"]
        user = "${VSPHERE_USERNAME}"
        password = "${VSPHERE_PASSWORD}"
        datacenters = "${VSPHERE_DATACENTER}"

        [Network]
        public-network = "${VSPHERE_NETWORK}"

    type: Opaque
type: addons.cluster.x-k8s.io/resource-set
----
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: cloud-provider-vsphere-credentials
  namespace: '${NAMESPACE}'
stringData:
  data: |-
    apiVersion: v1
    kind: Secret
    metadata:
      labels:
        component: cloud-controller-manager
        vsphere-cpi-infra: secret
      name: cloud-provider-vsphere-credentials
      namespace: kube-system
    stringData:
      ${VSPHERE_SERVER}.password: "${VSPHERE_PASSWORD}"
      ${VSPHERE_SERVER}.username: "${VSPHERE_USERNAME}"
    type: Opaque
type: addons.cluster.x-k8s.io/resource-set
----
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: v1
data:
  data: |-
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cloud-config
      labels:
        app: "cpi"
        vsphere-cpi-infra: cloud-config
        component: cloud-controller-manager
      namespace: kube-system
    data:
      vsphere.conf: |
        global:
          port: 443
          thumbprint: "${VSPHERE_TLS_THUMBPRINT}"
          secretName: cloud-provider-vsphere-credentials
          secretNamespace: kube-system
        vcenter:
          "${VSPHERE_SERVER}":
            server: "${VSPHERE_SERVER}"
            datacenters:
              - "${VSPHERE_DATACENTER}"
kind: ConfigMap
metadata:
  name: cpi-configmap
----
+
As the last object resource create ClusterResourceSet.
+
[source,yaml]
----
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: '${CLUSTER_NAME}'
  name: ${CLUSTER_NAME}-crs-0
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: '${CLUSTER_NAME}'
  resources:
  - kind: Secret
    name: vsphere-config-secret
  - kind: Secret
    name: cloud-provider-vsphere-credentials
  - kind: ConfigMap
    name: cpi-configmap
----
======

== Create a Cluster from a ClusterClass

[WARNING]
====
Examples using `HelmApps` need at least Rancher `v2.11`, or otherwise Fleet `v0.12` or higher.
====

[tabs]
======

Azure RKE2::
+
* An Azure ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/azure/clusterclass-rke2-example.yaml
----
+
* Additionally, the https://capz.sigs.k8s.io/self-managed/cloud-provider-config[Azure Cloud Provider] will need to be installed on each downstream Cluster, for the nodes to be initialized correctly. +
For this example we are also going to install https://docs.tigera.io/calico/latest/about/[Calico] as the default CNI. +
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/ccm/azure/helm-chart.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the Azure Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
Also beware that the `internal-first` `registrationMethod` variable is used as a workaround for correct provisioning. +
This immutable variable however will lead to issues when scaling or rolling out control plane nodes. +
A https://github.com/kubernetes-sigs/cluster-api-provider-azure/pull/5525[patch] will support this case in a future release of CAPZ, but the Cluster will need to be reprovisioned to change the `registrationMethod` +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
    cloud-provider: azure
    cni: calico
  name: azure-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-rke2-example
    controlPlane:
      replicas: 3
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    - name: registrationMethod
      value: internal-first
    version: v1.31.1+rke2r1
    workers:
      machineDeployments:
      - class: rke2-default-worker
        name: md-0
        replicas: 3
----

Azure AKS::
+
* An Azure AKS ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/azure/clusterclass-aks-example.yaml
----
+
* Create the Azure AKS Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
  name: azure-aks-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-aks-example
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    version: v1.31.1
    workers:
      machinePools:
      - class: default-system
        name: system-1
        replicas: 1
      - class: default-worker
        name: worker-1
        replicas: 1
----

Docker Kubeadm::
+
* A Docker Kubeadm ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/docker/clusterclass-docker-kubeadm.yaml
----
+
* For this example we are also going to install Calico as the default CNI.
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the Docker Kubeadm Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: docker-kubeadm-quickstart
  labels:
    cni: calico
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
        - 10.96.0.0/24
  topology:
    class: docker-kubeadm-example
    controlPlane:
      replicas: 3
    version: v1.31.6
    workers:
      machineDeployments:
        - class: default-worker
          name: md-0
          replicas: 3
----

Docker RKE2::
+
* A Docker RKE2 ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/docker/clusterclass-docker-rke2.yaml
----
+
* For this example we are also going to install Calico as the default CNI.
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the LoadBalancer ConfigMap for Docker RKEv2 Cluster +
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: docker-rke2-lb-config
  annotations:
    "helm.sh/resource-policy": keep
data:
  value: |-
    # generated by kind
    global
      log /dev/log local0
      log /dev/log local1 notice
      daemon
      # limit memory usage to approximately 18 MB
      # (see https://github.com/kubernetes-sigs/kind/pull/3115)
      maxconn 100000
    resolvers docker
      nameserver dns 127.0.0.11:53
    defaults
      log global
      mode tcp
      option dontlognull
      # TODO: tune these
      timeout connect 5000
      timeout client 50000
      timeout server 50000
      # allow to boot despite dns don't resolve backends
      default-server init-addr none
    frontend stats
      mode http
      bind *:8404
      stats enable
      stats uri /stats
      stats refresh 1s
      stats admin if TRUE
    frontend control-plane
      bind *:{{ .FrontendControlPlanePort }}
      {{ if .IPv6 -}}
      bind :::{{ .FrontendControlPlanePort }};
      {{- end }}
      default_backend kube-apiservers
    backend kube-apiservers
      option httpchk GET /healthz
      {{range $server, $backend := .BackendServers }}
      server {{ $server }} {{ JoinHostPort $backend.Address $.BackendControlPlanePort }} check check-ssl verify none resolvers docker resolve-prefer {{ if $.IPv6 -}} ipv6 {{- else -}} ipv4 {{- end }}
      {{- end}}
    frontend rke2-join
      bind *:9345
      {{ if .IPv6 -}}
      bind :::9345;
      {{- end }}
      default_backend rke2-servers
    backend rke2-servers
      option httpchk GET /v1-rke2/readyz
      http-check expect status 403
      {{range $server, $backend := .BackendServers }}
      server {{ $server }} {{ $backend.Address }}:9345 check check-ssl verify none
      {{- end}}
----
+
* Create the Docker Kubeadm Cluster from the example ClusterClass +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster 
metadata:
  name: docker-rke2-example
  labels:
    cni: calico
  annotations:
    cluster-api.cattle.io/upstream-system-agent: "true"
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
    services:
      cidrBlocks:
      - 10.96.0.0/24
    serviceDomain: cluster.local
  topology:
    class: docker-rke2-example
    controlPlane:
      replicas: 3
    variables:
    - name: rke2CNI
      value: none
    - name: dockerImage
      value: kindest/node:v1.31.6
    version: v1.31.6+rke2r1
    workers:
      machineDeployments:
      - class: default-worker
        name: md-0
        replicas: 3
----

vSphere Kubeadm::
+
* A vSphere ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/vsphere/clusterclass-kubeadm-example.yaml
----
+
* Additionally, the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere[vSphere Cloud Provider] will need to be installed on CAPI Cluster. +
For this example we are also going to install https://docs.tigera.io/calico/latest/about/[Calico] as the default CNI. +
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/ccm/vsphere/helm-chart.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the vSphere Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: vsphere-kubeadm-example
    cni: calico
    cloud-provider: vsphere
    csi: vsphere
  name: vsphere-kubeadm-example
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: 'vsphere-kubeadm-example'
    controlPlane:
      replicas: 3
    variables:
    - name: sshKey
      value: '${VSPHERE_SSH_AUTHORIZED_KEY}'
    - name: kubeVipPodManifest
      value: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-vip
          namespace: kube-system
        spec:
          containers:
          - args:
            - manager
            env:
            - name: vip_arp
              value: "true"
            - name: port
              value: "6443"
            - name: vip_nodename
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: vip_interface
              value: ${VIP_NETWORK_INTERFACE:=""}
            - name: vip_cidr
              value: "32"
            - name: dns_mode
              value: first
            - name: cp_enable
              value: "true"
            - name: cp_namespace
              value: kube-system
            - name: svc_enable
              value: "true"
            - name: svc_leasename
              value: plndr-svcs-lock
            - name: vip_leaderelection
              value: "true"
            - name: vip_leasename
              value: plndr-cp-lock
            - name: vip_leaseduration
              value: "5"
            - name: vip_renewdeadline
              value: "3"
            - name: vip_retryperiod
              value: "1"
            - name: address
              value: ${CONTROL_PLANE_ENDPOINT_IP}
            - name: prometheus_server
              value: :2112
            image: ghcr.io/kube-vip/kube-vip:v0.8.10
            imagePullPolicy: IfNotPresent
            name: kube-vip
            resources: {}
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
                drop:
                - ALL
            volumeMounts:
            - mountPath: /etc/kubernetes/admin.conf
              name: kubeconfig
            - mountPath: /etc/hosts
              name: etchosts
          hostNetwork: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/super-admin.conf
              type: File
            name: kubeconfig
          - hostPath:
              path: /etc/kube-vip.hosts
              type: File
            name: etchosts
    - name: controlPlaneIpAddr
      value: ${CONTROL_PLANE_ENDPOINT_IP}
    - name: controlPlanePort
      value: 6443
    - name: infraServer
      value:
        thumbprint: '${VSPHERE_TLS_THUMBPRINT}'
        url: '${VSPHERE_SERVER}'
    - name: credsSecretName
      value: vsphere-cluster-secret
    - name: vsphereDataCenter
      value: '${VSPHERE_DATACENTER}'
    - name: vsphereDataStore
      value: '${VSPHERE_DATASTORE}'
    - name: vsphereFolder
      value: '${VSPHERE_FOLDER}'
    - name: vsphereNetwork
      value: '${VSPHERE_NETWORK}'
    - name: vsphereResourcePool
      value: '${VSPHERE_RESOURCE_POOL}'
    - name: vsphereServer
      value: '${VSPHERE_SERVER}'
    - name: vsphereStorageServer
      value: '${VSPHERE_STORAGE_SERVER}'
    - name: vsphereTemplate
      value: '${VSPHERE_TEMPLATE}'
    version: '${KUBERNETES_VERSION}'
    workers:
      machineDeployments:
      - class: vsphere-kubeadm-example-worker
        metadata: {}
        name: md-0
        replicas: 3
----

vSphere RKE2::
+
* A vSphere ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/vsphere/clusterclass-rke2-example.yaml
----
+
* Additionally, the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere[vSphere Cloud Provider] will need to be installed on CAPI Cluster. +
For this example we are also going to install https://docs.tigera.io/calico/latest/about/[Calico] as the default CNI. +
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/ccm/vsphere/helm-chart.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the vSphere Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: vsphere-rke2-example
  name: vsphere-rke2-example
spec:
  topology:
    class: vsphere-rke2-example
    version: v1.31.7-rke2r1
    controlPlane:
      replicas: 3
    workers:
      machineDeployments:
      - class: vsphere-rke2-example-worker
        name: md-0
        replicas: 3
    variables:
    - name: credsSecretName
      value: vsphere-cluster-secret
    - name: infraServer
      value:
        url: '${VSPHERE_SERVER}'
        thumbprint: '${VSPHERE_TLS_THUMBPRINT}'
    - name: vsphereDataCenter
      value: '${VSPHERE_DATACENTER}'
    - name: vsphereDataStore
      value: '${VSPHERE_DATASTORE}'
    - name: vsphereFolder
      value: '${VSPHERE_FOLDER}'
    - name: vsphereNetwork
      value: '${VSPHERE_NETWORK}'
    - name: vsphereResourcePool
      value: '${VSPHERE_RESOURCE_POOL}'
    - name: vsphereServer
      value: '${VSPHERE_SERVER}'
    - name: vsphereTemplate
      value: '${VSPHERE_TEMPLATE}'
    - name: controlPlaneIpAddr
      value: '${CONTROL_PLANE_ENDPOINT_IP}'
    - name: controlPlanePort
      value: 6443
    - name: kubeVipPodManifest
      value: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kube-vip
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          name: system:kube-vip-role
        rules:
          - apiGroups: [""]
            resources: ["services", "services/status", "nodes"]
            verbs: ["list","get","watch", "update"]
          - apiGroups: ["coordination.k8s.io"]
            resources: ["leases"]
            verbs: ["list", "get", "watch", "update", "create"]
        ---
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
          name: system:kube-vip-binding
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:kube-vip-role
        subjects:
        - kind: ServiceAccount
          name: kube-vip
          namespace: kube-system
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          creationTimestamp: null
          name: kube-vip
          namespace: kube-system
        spec:
          containers:
          - args:
            - manager
            env:
            - name: vip_arp
              value: "true"
            - name: port
              value: "6443"
            - name: vip_nodename
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: vip_interface
              value: eth0
            - name: vip_cidr
              value: "32"
            - name: dns_mode
              value: first
            - name: cp_enable
              value: "true"
            - name: cp_namespace
              value: kube-system
            - name: svc_enable
              value: "true"
            - name: svc_leasename
              value: plndr-svcs-lock
            - name: vip_leaderelection
              value: "true"
            - name: vip_leasename
              value: plndr-cp-lock
            - name: vip_leaseduration
              value: "5"
            - name: vip_renewdeadline
              value: "3"
            - name: vip_retryperiod
              value: "1"
            - name: address
              value: ${CONTROL_PLANE_ENDPOINT_IP}
            - name: prometheus_server
              value: :2112
            image: ghcr.io/kube-vip/kube-vip:v0.8.10
            imagePullPolicy: IfNotPresent
            name: kube-vip
            resources: {}
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
                drop:
                - ALL
            volumeMounts:
            - mountPath: /etc/kubernetes/admin.conf
              name: kubeconfig
          hostAliases:
          - hostnames:
            - kubernetes
            ip: 127.0.0.1
          hostNetwork: true
          serviceAccountName: kube-vip
          volumes:
          - hostPath:
              path: /etc/rancher/rke2/rke2.yaml
              type: File
            name: kubeconfig
----
======
