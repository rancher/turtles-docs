= ClusterClass

In this section we cover using https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/[ClusterClass] with {product_name}.

== Prerequisites

* Rancher Manager cluster with {product_name} installed

== Setup

[tabs]
======

Azure::
+
To prepare the management Cluster, we are going to install the https://capz.sigs.k8s.io/[Cluster API Provider Azure], and create a https://capz.sigs.k8s.io/topics/identities#service-principal[ServicePrincipal] identity to provision a new Cluster on Azure. +
Before we start, a ServicePrincipal needs to be created, with at least Contributor access to an Azure subscription. +
Refer to the https://capz.sigs.k8s.io/getting-started#setting-up-your-azure-environment[CAPZ documentation] for more details. +
+
* Provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capz-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: azure
  namespace: capz-system
spec:
  type: infrastructure
  name: azure
----
+
* Identity setup
+
A Secret containing the ADD Service Principal password need to be created first.  
+
[source,bash]
----
# Settings needed for AzureClusterIdentity used by the AzureCluster
export AZURE_CLUSTER_IDENTITY_SECRET_NAME="cluster-identity-secret"
export AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE="default"
export AZURE_CLIENT_SECRET="<Password>"

# Create a secret to include the password of the Service Principal identity created in Azure
# This secret will be referenced by the AzureClusterIdentity used by the AzureCluster
kubectl create secret generic "${AZURE_CLUSTER_IDENTITY_SECRET_NAME}" --from-literal=clientSecret="${AZURE_CLIENT_SECRET}" --namespace "${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}"
----
+
The AzureClusterIdentity can now be created to use the Service Principal identity. +
Note that the AzureClusterIdentity is a namespaced resource and it needs to be created in the same namespace as the Cluster. +
For more information on best practices when using Azure identities, please refer to the official https://capz.sigs.k8s.io/topics/identities-use-cases[documentation]. +
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureClusterIdentity
metadata:
  labels:
    clusterctl.cluster.x-k8s.io/move-hierarchy: "true"
  name: cluster-identity
spec:
  allowedNamespaces: {}
  clientID: <AZURE_APP_ID>
  clientSecret:
    name: <AZURE_CLUSTER_IDENTITY_SECRET_NAME>
    namespace: <AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE>
  tenantID: <AZURE_TENANT_ID>
  type: ServicePrincipal
----

Docker::
+
To prepare the management Cluster, we are going to install the Docker Cluster API Provider.
+
* Infrastructure Docker provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capd-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: docker
  namespace: capd-system
spec:
  type: infrastructure
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----

vSphere::
+
To prepare Rancher management Cluster, we are going to install the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere[vSphere Cluster API Provider]. +
It is an example of vSphere provider installation, follow the provider documentation if some options need to be customized. +
+
* Infrastructure vSphere provider installation
+
[source, yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  name: capv-system
---
apiVersion: v1
kind: Secret
metadata:
  name: vsphere
  namespace: capv-system
type: Opaque
stringData:
  VSPHERE_USERNAME: xxx
  VSPHERE_PASSWORD: xxx
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: vsphere
  namespace: capv-system
spec:
  type: infrastructure
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----
======

== Create a Cluster from a ClusterClass

[WARNING]
====
Examples using `HelmApps` need at least Rancher `v2.11`, or otherwise Fleet `v0.12` or higher.
====

[tabs]
======

Azure RKE2::
+
* An Azure ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/azure/clusterclass-rke2-example.yaml
----
+
* Additionally, the https://capz.sigs.k8s.io/self-managed/cloud-provider-config[Azure Cloud Provider] will need to be installed on each downstream Cluster, for the nodes to be initialized correctly. +
For this example we are also going to install https://docs.tigera.io/calico/latest/about/[Calico] as the default CNI. +
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/ccm/azure/helm-chart.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the Azure Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
Also beware that the `internal-first` `registrationMethod` variable is used as a workaround for correct provisioning. +
This immutable variable however will lead to issues when scaling or rolling out control plane nodes. +
A https://github.com/kubernetes-sigs/cluster-api-provider-azure/pull/5525[patch] will support this case in a future release of CAPZ, but the Cluster will need to be reprovisioned to change the `registrationMethod` +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
    cloud-provider: azure
    cni: calico
  name: azure-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-rke2-example
    controlPlane:
      replicas: 3
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    - name: registrationMethod
      value: internal-first
    version: v1.31.1+rke2r1
    workers:
      machineDeployments:
      - class: rke2-default-worker
        name: md-0
        replicas: 3
----

Azure AKS::
+
* An Azure AKS ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/azure/clusterclass-aks-example.yaml
----
+
* Create the Azure AKS Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
  name: azure-aks-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-aks-example
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    version: v1.31.1
    workers:
      machinePools:
      - class: default-system
        name: system-1
        replicas: 1
      - class: default-worker
        name: worker-1
        replicas: 1
----

Docker Kubeadm::
+
* A Docker Kubeadm ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/docker/clusterclass-docker-kubeadm.yaml
----
+
* For this example we are also going to install Calico as the default CNI.
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the Docker Kubeadm Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: docker-kubeadm-quickstart
  labels:
    cni: calico
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
        - 10.96.0.0/24
  topology:
    class: docker-kubeadm-example
    controlPlane:
      replicas: 3
    version: v1.31.6
    workers:
      machineDeployments:
        - class: default-worker
          name: md-0
          replicas: 3
----

Docker RKE2::
+
* A Docker RKE2 ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/docker/clusterclass-docker-rke2.yaml
----
+
* For this example we are also going to install Calico as the default CNI.
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the LoadBalancer ConfigMap for Docker RKEv2 Cluster +
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: docker-rke2-lb-config
  annotations:
    "helm.sh/resource-policy": keep
data:
  value: |-
    # generated by kind
    global
      log /dev/log local0
      log /dev/log local1 notice
      daemon
      # limit memory usage to approximately 18 MB
      # (see https://github.com/kubernetes-sigs/kind/pull/3115)
      maxconn 100000
    resolvers docker
      nameserver dns 127.0.0.11:53
    defaults
      log global
      mode tcp
      option dontlognull
      # TODO: tune these
      timeout connect 5000
      timeout client 50000
      timeout server 50000
      # allow to boot despite dns don't resolve backends
      default-server init-addr none
    frontend stats
      mode http
      bind *:8404
      stats enable
      stats uri /stats
      stats refresh 1s
      stats admin if TRUE
    frontend control-plane
      bind *:{{ .FrontendControlPlanePort }}
      {{ if .IPv6 -}}
      bind :::{{ .FrontendControlPlanePort }};
      {{- end }}
      default_backend kube-apiservers
    backend kube-apiservers
      option httpchk GET /healthz
      {{range $server, $backend := .BackendServers }}
      server {{ $server }} {{ JoinHostPort $backend.Address $.BackendControlPlanePort }} check check-ssl verify none resolvers docker resolve-prefer {{ if $.IPv6 -}} ipv6 {{- else -}} ipv4 {{- end }}
      {{- end}}
    frontend rke2-join
      bind *:9345
      {{ if .IPv6 -}}
      bind :::9345;
      {{- end }}
      default_backend rke2-servers
    backend rke2-servers
      option httpchk GET /v1-rke2/readyz
      http-check expect status 403
      {{range $server, $backend := .BackendServers }}
      server {{ $server }} {{ $backend.Address }}:9345 check check-ssl verify none
      {{- end}}
----
+
* Create the Docker Kubeadm Cluster from the example ClusterClass +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster 
metadata:
  name: docker-rke2-example
  labels:
    cni: calico
  annotations:
    cluster-api.cattle.io/upstream-system-agent: "true"
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
    services:
      cidrBlocks:
      - 10.96.0.0/24
    serviceDomain: cluster.local
  topology:
    class: docker-rke2-example
    controlPlane:
      replicas: 3
    variables:
    - name: rke2CNI
      value: none
    - name: dockerImage
      value: kindest/node:v1.31.6
    version: v1.31.6+rke2r1
    workers:
      machineDeployments:
      - class: default-worker
        name: md-0
        replicas: 3
----

vSphere RKE2::
+
* A vSphere ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/clusterclasses/vsphere/clusterclass-rke2-example.yaml
----
+
* Additionally, the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere[vSphere Cloud Provider] will need to be installed on CAPI Cluster. +
For this example we are also going to install https://docs.tigera.io/calico/latest/about/[Calico] as the default CNI. +
+
We can do this automatically at Cluster creation using the https://rancher-sandbox.github.io/cluster-api-addon-provider-fleet/[Cluster API Add-on Provider Fleet]. +
This Add-on provider is installed by default with {product_name}. +
Two `HelmApps` need to be created first, to be applied on the new Cluster via label selectors. +
+
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/ccm/vsphere/helm-chart.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/turtles/refs/heads/main/examples/applications/cni/calico/helm-chart.yaml
----
+
* Create the vSphere Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: vsphere-rke2-example
  name: vsphere-rke2-example
spec:
  topology:
    class: vsphere-rke2-example
    version: ${RKE2_VERSION}
    controlPlane:
      replicas: 3
    workers:
      machineDeployments:
      - class: vsphere-rke2-example-worker
        name: md-0
        replicas: 3
        metadata: {}
    variables:
    - name: credsSecretName
      value: vsphere-rke2-example
    - name: infraServer
      value:
        url: '${VSPHERE_SERVER}'
        thumbprint: '${VSPHERE_TLS_THUMBPRINT}'
    - name: vsphereDataCenter
      value: '${VSPHERE_DATACENTER}'
    - name: vsphereDataStore
      value: '${VSPHERE_DATASTORE}'
    - name: vsphereFolder
      value: '${VSPHERE_FOLDER}'
    - name: vsphereNetwork
      value: '${VSPHERE_NETWORK}'
    - name: vsphereResourcePool
      value: '${VSPHERE_RESOURCE_POOL}'
    - name: vsphereServer
      value: '${VSPHERE_SERVER}'
    - name: vsphereTemplate
      value: '${VSPHERE_TEMPLATE}'
    - name: controlPlaneIpAddr
      value: '${CONTROL_PLANE_ENDPOINT_IP}'
    - name: controlPlanePort
      value: 6443
    - name: kubeVipPodManifest
      value: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kube-vip
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          name: system:kube-vip-role
        rules:
          - apiGroups: [""]
            resources: ["services", "services/status", "nodes"]
            verbs: ["list","get","watch", "update"]
          - apiGroups: ["coordination.k8s.io"]
            resources: ["leases"]
            verbs: ["list", "get", "watch", "update", "create"]
        ---
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
          name: system:kube-vip-binding
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:kube-vip-role
        subjects:
        - kind: ServiceAccount
          name: kube-vip
          namespace: kube-system
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-vip
          namespace: kube-system
        spec:
          tolerations:
          - effect: NoSchedule
            key: node.cloudprovider.kubernetes.io/uninitialized
            operator: Exists
          containers:
          - args:
            - manager
            env:
            - name: vip_arp
              value: "true"
            - name: port
              value: "6443"
            - name: vip_nodename
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: vip_interface
              value: eth0
            - name: vip_cidr
              value: "32"
            - name: dns_mode
              value: first
            - name: cp_enable
              value: "true"
            - name: cp_namespace
              value: kube-system
            - name: svc_enable
              value: "true"
            - name: svc_leasename
              value: plndr-svcs-lock
            - name: vip_leaderelection
              value: "true"
            - name: vip_leasename
              value: plndr-cp-lock
            - name: vip_leaseduration
              value: "5"
            - name: vip_renewdeadline
              value: "3"
            - name: vip_retryperiod
              value: "1"
            - name: address
              value: ${CONTROL_PLANE_ENDPOINT_IP}
            - name: prometheus_server
              value: :2112
            image: ghcr.io/kube-vip/kube-vip:v0.8.10
            imagePullPolicy: IfNotPresent
            name: kube-vip
            resources: {}
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
                drop:
                - ALL
            volumeMounts:
            - mountPath: /etc/kubernetes/admin.conf
              name: kubeconfig
          hostAliases:
          - hostnames:
            - kubernetes
            ip: 127.0.0.1
          hostNetwork: true
          serviceAccountName: kube-vip
          volumes:
          - hostPath:
              path: /etc/rancher/rke2/rke2.yaml
              type: File
            name: kubeconfig
---
apiVersion: v1
kind: Secret
metadata:
  name: vsphere-rke2-example
stringData:
  password: '${VSPHERE_PASSWORD}'
  username: '${VSPHERE_USERNAME}'
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: vsphere-rke2-example
  name: 'vsphere-rke2-example-crs-0'
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: '${CLUSTER_NAME}'
  resources:
  - kind: Secret
    name: vsphere-config-secret
  - kind: ConfigMap
    name: csi-manifests
  - kind: Secret
    name: cloud-provider-vsphere-credentials
  - kind: ConfigMap
    name: cpi-manifests
---
apiVersion: v1
kind: Secret
metadata:
  name: vsphere-config-secret
  namespace: '${NAMESPACE}'
stringData:
  data: |-
    apiVersion: v1
    kind: Secret
    metadata:
      name: vsphere-config-secret
      namespace: vmware-system-csi
    stringData:
      csi-vsphere.conf: |+
        [Global]
        thumbprint = "${VSPHERE_TLS_THUMBPRINT}"

        [VirtualCenter "${VSPHERE_SERVER}"]
        user = "${VSPHERE_USERNAME}"
        password = "${VSPHERE_PASSWORD}"
        datacenters = "${VSPHERE_DATACENTER}"

        [Network]
        public-network = "${VSPHERE_NETWORK}"

    type: Opaque
type: addons.cluster.x-k8s.io/resource-set
---
apiVersion: v1
data:
  data: |-
    apiVersion: v1
    kind: Namespace
    metadata:
      name: vmware-system-csi
    ---
    apiVersion: storage.k8s.io/v1
    kind: CSIDriver
    metadata:
      name: csi.vsphere.vmware.com
    spec:
      attachRequired: true
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: vsphere-csi-controller
      namespace: vmware-system-csi
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: vsphere-csi-controller-role
    rules:
    - apiGroups:
      - ""
      resources:
      - nodes
      - pods
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - get
      - list
      - watch
      - create
    - apiGroups:
      - ""
      resources:
      - persistentvolumeclaims
      verbs:
      - get
      - list
      - watch
      - update
    - apiGroups:
      - ""
      resources:
      - persistentvolumeclaims/status
      verbs:
      - patch
    - apiGroups:
      - ""
      resources:
      - persistentvolumes
      verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
      - patch
    - apiGroups:
      - ""
      resources:
      - events
      verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
    - apiGroups:
      - coordination.k8s.io
      resources:
      - leases
      verbs:
      - get
      - watch
      - list
      - delete
      - update
      - create
    - apiGroups:
      - storage.k8s.io
      resources:
      - storageclasses
      - csinodes
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - storage.k8s.io
      resources:
      - volumeattachments
      verbs:
      - get
      - list
      - watch
      - patch
    - apiGroups:
      - cns.vmware.com
      resources:
      - triggercsifullsyncs
      verbs:
      - create
      - get
      - update
      - watch
      - list
    - apiGroups:
      - cns.vmware.com
      resources:
      - cnsvspherevolumemigrations
      verbs:
      - create
      - get
      - list
      - watch
      - update
      - delete
    - apiGroups:
      - cns.vmware.com
      resources:
      - cnsvolumeinfoes
      verbs:
      - create
      - get
      - list
      - watch
      - delete
    - apiGroups:
      - apiextensions.k8s.io
      resources:
      - customresourcedefinitions
      verbs:
      - get
      - create
      - update
    - apiGroups:
      - storage.k8s.io
      resources:
      - volumeattachments/status
      verbs:
      - patch
    - apiGroups:
      - cns.vmware.com
      resources:
      - cnsvolumeoperationrequests
      verbs:
      - create
      - get
      - list
      - update
      - delete
    - apiGroups:
      - snapshot.storage.k8s.io
      resources:
      - volumesnapshots
      verbs:
      - get
      - list
    - apiGroups:
      - snapshot.storage.k8s.io
      resources:
      - volumesnapshotclasses
      verbs:
      - watch
      - get
      - list
    - apiGroups:
      - snapshot.storage.k8s.io
      resources:
      - volumesnapshotcontents
      verbs:
      - create
      - get
      - list
      - watch
      - update
      - delete
      - patch
    - apiGroups:
      - snapshot.storage.k8s.io
      resources:
      - volumesnapshotcontents/status
      verbs:
      - update
      - patch
    - apiGroups:
      - cns.vmware.com
      resources:
      - csinodetopologies
      verbs:
      - get
      - update
      - watch
      - list
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: vsphere-csi-controller-binding
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: vsphere-csi-controller-role
    subjects:
    - kind: ServiceAccount
      name: vsphere-csi-controller
      namespace: vmware-system-csi
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: vsphere-csi-node
      namespace: vmware-system-csi
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: vsphere-csi-node-cluster-role
    rules:
    - apiGroups:
      - cns.vmware.com
      resources:
      - csinodetopologies
      verbs:
      - create
      - watch
      - get
      - patch
    - apiGroups:
      - ""
      resources:
      - nodes
      verbs:
      - get
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: vsphere-csi-node-cluster-role-binding
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: vsphere-csi-node-cluster-role
    subjects:
    - kind: ServiceAccount
      name: vsphere-csi-node
      namespace: vmware-system-csi
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: vsphere-csi-node-role
      namespace: vmware-system-csi
    rules:
    - apiGroups:
      - ""
      resources:
      - configmaps
      verbs:
      - get
      - list
      - watch
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: vsphere-csi-node-binding
      namespace: vmware-system-csi
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: vsphere-csi-node-role
    subjects:
    - kind: ServiceAccount
      name: vsphere-csi-node
      namespace: vmware-system-csi
    ---
    apiVersion: v1
    data:
      pv-to-backingdiskobjectid-mapping: "false"
      trigger-csi-fullsync: "false"
    kind: ConfigMap
    metadata:
      name: internal-feature-states.csi.vsphere.vmware.com
      namespace: vmware-system-csi
    ---
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: vsphere-csi-controller
      name: vsphere-csi-controller
      namespace: vmware-system-csi
    spec:
      ports:
      - name: ctlr
        port: 2112
        protocol: TCP
        targetPort: 2112
      - name: syncer
        port: 2113
        protocol: TCP
        targetPort: 2113
      selector:
        app: vsphere-csi-controller
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: vsphere-csi-controller
      namespace: vmware-system-csi
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: vsphere-csi-controller
      strategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: vsphere-csi-controller
            role: vsphere-csi
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
                - matchExpressions:
                  - key: node-role.kubernetes.io/controlplane
                    operator: Exists
                - matchExpressions:
                  - key: node-role.kubernetes.io/master
                    operator: Exists
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - vsphere-csi-controller
                topologyKey: kubernetes.io/hostname
          containers:
          - args:
            - --v=4
            - --timeout=300s
            - --csi-address=$(ADDRESS)
            - --leader-election
            - --leader-election-lease-duration=120s
            - --leader-election-renew-deadline=60s
            - --leader-election-retry-period=30s
            - --kube-api-qps=100
            - --kube-api-burst=100
            env:
            - name: ADDRESS
              value: /csi/csi.sock
            image: registry.k8s.io/sig-storage/csi-attacher:v4.5.1
            name: csi-attacher
            volumeMounts:
            - mountPath: /csi
              name: socket-dir
          - args:
            - --v=4
            - --timeout=300s
            - --handle-volume-inuse-error=false
            - --csi-address=$(ADDRESS)
            - --kube-api-qps=100
            - --kube-api-burst=100
            - --leader-election
            - --leader-election-lease-duration=120s
            - --leader-election-renew-deadline=60s
            - --leader-election-retry-period=30s
            env:
            - name: ADDRESS
              value: /csi/csi.sock
            image: registry.k8s.io/sig-storage/csi-resizer:v1.10.1
            name: csi-resizer
            volumeMounts:
            - mountPath: /csi
              name: socket-dir
          - args:
            - --fss-name=internal-feature-states.csi.vsphere.vmware.com
            - --fss-namespace=$(CSI_NAMESPACE)
            env:
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
            - name: X_CSI_MODE
              value: controller
            - name: X_CSI_SPEC_DISABLE_LEN_CHECK
              value: "true"
            - name: X_CSI_SERIAL_VOL_ACCESS_TIMEOUT
              value: 3m
            - name: VSPHERE_CSI_CONFIG
              value: /etc/cloud/csi-vsphere.conf
            - name: LOGGER_LEVEL
              value: PRODUCTION
            - name: INCLUSTER_CLIENT_QPS
              value: "100"
            - name: INCLUSTER_CLIENT_BURST
              value: "100"
            - name: CSI_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            image: registry.k8s.io/csi-vsphere/driver:v3.3.1
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /healthz
                port: healthz
              initialDelaySeconds: 30
              periodSeconds: 180
              timeoutSeconds: 10
            name: vsphere-csi-controller
            ports:
            - containerPort: 9808
              name: healthz
              protocol: TCP
            - containerPort: 2112
              name: prometheus
              protocol: TCP
            securityContext:
              runAsGroup: 65532
              runAsNonRoot: true
              runAsUser: 65532
            volumeMounts:
            - mountPath: /etc/cloud
              name: vsphere-config-volume
              readOnly: true
            - mountPath: /csi
              name: socket-dir
          - args:
            - --v=4
            - --csi-address=/csi/csi.sock
            image: registry.k8s.io/sig-storage/livenessprobe:v2.12.0
            name: liveness-probe
            volumeMounts:
            - mountPath: /csi
              name: socket-dir
          - args:
            - --leader-election
            - --leader-election-lease-duration=30s
            - --leader-election-renew-deadline=20s
            - --leader-election-retry-period=10s
            - --fss-name=internal-feature-states.csi.vsphere.vmware.com
            - --fss-namespace=$(CSI_NAMESPACE)
            env:
            - name: FULL_SYNC_INTERVAL_MINUTES
              value: "30"
            - name: VSPHERE_CSI_CONFIG
              value: /etc/cloud/csi-vsphere.conf
            - name: LOGGER_LEVEL
              value: PRODUCTION
            - name: INCLUSTER_CLIENT_QPS
              value: "100"
            - name: INCLUSTER_CLIENT_BURST
              value: "100"
            - name: CSI_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            image: registry.k8s.io/csi-vsphere/syncer:v3.3.1
            imagePullPolicy: Always
            name: vsphere-syncer
            ports:
            - containerPort: 2113
              name: prometheus
              protocol: TCP
            securityContext:
              runAsGroup: 65532
              runAsNonRoot: true
              runAsUser: 65532
            volumeMounts:
            - mountPath: /etc/cloud
              name: vsphere-config-volume
              readOnly: true
          - args:
            - --v=4
            - --timeout=300s
            - --csi-address=$(ADDRESS)
            - --kube-api-qps=100
            - --kube-api-burst=100
            - --leader-election
            - --leader-election-lease-duration=120s
            - --leader-election-renew-deadline=60s
            - --leader-election-retry-period=30s
            - --default-fstype=ext4
            env:
            - name: ADDRESS
              value: /csi/csi.sock
            image: registry.k8s.io/sig-storage/csi-provisioner:v4.0.1
            name: csi-provisioner
            volumeMounts:
            - mountPath: /csi
              name: socket-dir
          - args:
            - --v=4
            - --kube-api-qps=100
            - --kube-api-burst=100
            - --timeout=300s
            - --csi-address=$(ADDRESS)
            - --leader-election
            - --leader-election-lease-duration=120s
            - --leader-election-renew-deadline=60s
            - --leader-election-retry-period=30s
            env:
            - name: ADDRESS
              value: /csi/csi.sock
            image: registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2
            name: csi-snapshotter
            volumeMounts:
            - mountPath: /csi
              name: socket-dir
          dnsPolicy: Default
          priorityClassName: system-cluster-critical
          serviceAccountName: vsphere-csi-controller
          tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
            operator: Exists
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
            operator: Exists
          volumes:
          - name: vsphere-config-volume
            secret:
              secretName: vsphere-config-secret
          - emptyDir: {}
            name: socket-dir
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: vsphere-csi-node
      namespace: vmware-system-csi
    spec:
      selector:
        matchLabels:
          app: vsphere-csi-node
      template:
        metadata:
          labels:
            app: vsphere-csi-node
            role: vsphere-csi
        spec:
          containers:
          - args:
            - --v=5
            - --csi-address=$(ADDRESS)
            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
            env:
            - name: ADDRESS
              value: /csi/csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/csi.vsphere.vmware.com/csi.sock
            image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1
            livenessProbe:
              exec:
                command:
                - /csi-node-driver-registrar
                - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.vsphere.vmware.com/csi.sock
                - --mode=kubelet-registration-probe
              initialDelaySeconds: 3
            name: node-driver-registrar
            volumeMounts:
            - mountPath: /csi
              name: plugin-dir
            - mountPath: /registration
              name: registration-dir
          - args:
            - --fss-name=internal-feature-states.csi.vsphere.vmware.com
            - --fss-namespace=$(CSI_NAMESPACE)
            env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
            - name: MAX_VOLUMES_PER_NODE
              value: "59"
            - name: X_CSI_MODE
              value: node
            - name: X_CSI_SPEC_REQ_VALIDATION
              value: "false"
            - name: X_CSI_SPEC_DISABLE_LEN_CHECK
              value: "true"
            - name: LOGGER_LEVEL
              value: PRODUCTION
            - name: CSI_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODEGETINFO_WATCH_TIMEOUT_MINUTES
              value: "1"
            image: registry.k8s.io/csi-vsphere/driver:v3.3.1
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /healthz
                port: healthz
              initialDelaySeconds: 10
              periodSeconds: 5
              timeoutSeconds: 5
            name: vsphere-csi-node
            ports:
            - containerPort: 9808
              name: healthz
              protocol: TCP
            securityContext:
              allowPrivilegeEscalation: true
              capabilities:
                add:
                - SYS_ADMIN
              privileged: true
            volumeMounts:
            - mountPath: /csi
              name: plugin-dir
            - mountPath: /var/lib/kubelet
              mountPropagation: Bidirectional
              name: pods-mount-dir
            - mountPath: /dev
              name: device-dir
            - mountPath: /sys/block
              name: blocks-dir
            - mountPath: /sys/devices
              name: sys-devices-dir
          - args:
            - --v=4
            - --csi-address=/csi/csi.sock
            image: registry.k8s.io/sig-storage/livenessprobe:v2.12.0
            name: liveness-probe
            volumeMounts:
            - mountPath: /csi
              name: plugin-dir
          dnsPolicy: ClusterFirstWithHostNet
          hostNetwork: true
          nodeSelector:
            kubernetes.io/os: linux
          priorityClassName: system-node-critical
          serviceAccountName: vsphere-csi-node
          tolerations:
          - effect: NoExecute
            operator: Exists
          - effect: NoSchedule
            operator: Exists
          volumes:
          - hostPath:
              path: /var/lib/kubelet/plugins_registry
              type: Directory
            name: registration-dir
          - hostPath:
              path: /var/lib/kubelet/plugins/csi.vsphere.vmware.com
              type: DirectoryOrCreate
            name: plugin-dir
          - hostPath:
              path: /var/lib/kubelet
              type: Directory
            name: pods-mount-dir
          - hostPath:
              path: /dev
            name: device-dir
          - hostPath:
              path: /sys/block
              type: Directory
            name: blocks-dir
          - hostPath:
              path: /sys/devices
              type: Directory
            name: sys-devices-dir
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: vsphere-csi-node-windows
      namespace: vmware-system-csi
    spec:
      selector:
        matchLabels:
          app: vsphere-csi-node-windows
      template:
        metadata:
          labels:
            app: vsphere-csi-node-windows
            role: vsphere-csi-windows
        spec:
          containers:
          - args:
            - --v=5
            - --csi-address=$(ADDRESS)
            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
            env:
            - name: ADDRESS
              value: unix://C:\\csi\\csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: C:\\var\\lib\\kubelet\\plugins\\csi.vsphere.vmware.com\\csi.sock
            image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0
            livenessProbe:
              exec:
                command:
                - /csi-node-driver-registrar.exe
                - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi.vsphere.vmware.com\\csi.sock
                - --mode=kubelet-registration-probe
              initialDelaySeconds: 3
            name: node-driver-registrar
            volumeMounts:
            - mountPath: /csi
              name: plugin-dir
            - mountPath: /registration
              name: registration-dir
          - args:
            - --fss-name=internal-feature-states.csi.vsphere.vmware.com
            - --fss-namespace=$(CSI_NAMESPACE)
            env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: CSI_ENDPOINT
              value: unix://C:\\csi\\csi.sock
            - name: MAX_VOLUMES_PER_NODE
              value: "59"
            - name: X_CSI_MODE
              value: node
            - name: X_CSI_SPEC_REQ_VALIDATION
              value: "false"
            - name: X_CSI_SPEC_DISABLE_LEN_CHECK
              value: "true"
            - name: LOGGER_LEVEL
              value: PRODUCTION
            - name: X_CSI_LOG_LEVEL
              value: DEBUG
            - name: CSI_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODEGETINFO_WATCH_TIMEOUT_MINUTES
              value: "1"
            image: registry.k8s.io/csi-vsphere/driver:v3.3.1
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /healthz
                port: healthz
              initialDelaySeconds: 10
              periodSeconds: 5
              timeoutSeconds: 5
            name: vsphere-csi-node
            ports:
            - containerPort: 9808
              name: healthz
              protocol: TCP
            volumeMounts:
            - mountPath: C:\csi
              name: plugin-dir
            - mountPath: C:\var\lib\kubelet
              name: pods-mount-dir
            - mountPath: \\.\pipe\csi-proxy-volume-v1
              name: csi-proxy-volume-v1
            - mountPath: \\.\pipe\csi-proxy-filesystem-v1
              name: csi-proxy-filesystem-v1
            - mountPath: \\.\pipe\csi-proxy-disk-v1
              name: csi-proxy-disk-v1
            - mountPath: \\.\pipe\csi-proxy-system-v1alpha1
              name: csi-proxy-system-v1alpha1
          - args:
            - --v=4
            - --csi-address=/csi/csi.sock
            image: registry.k8s.io/sig-storage/livenessprobe:v2.12.0
            name: liveness-probe
            volumeMounts:
            - mountPath: /csi
              name: plugin-dir
          nodeSelector:
            kubernetes.io/os: windows
          priorityClassName: system-node-critical
          serviceAccountName: vsphere-csi-node
          tolerations:
          - effect: NoExecute
            operator: Exists
          - effect: NoSchedule
            operator: Exists
          volumes:
          - hostPath:
              path: C:\var\lib\kubelet\plugins_registry\
              type: Directory
            name: registration-dir
          - hostPath:
              path: C:\var\lib\kubelet\plugins\csi.vsphere.vmware.com\
              type: DirectoryOrCreate
            name: plugin-dir
          - hostPath:
              path: \var\lib\kubelet
              type: Directory
            name: pods-mount-dir
          - hostPath:
              path: \\.\pipe\csi-proxy-disk-v1
              type: ""
            name: csi-proxy-disk-v1
          - hostPath:
              path: \\.\pipe\csi-proxy-volume-v1
              type: ""
            name: csi-proxy-volume-v1
          - hostPath:
              path: \\.\pipe\csi-proxy-filesystem-v1
              type: ""
            name: csi-proxy-filesystem-v1
          - hostPath:
              path: \\.\pipe\csi-proxy-system-v1alpha1
              type: ""
            name: csi-proxy-system-v1alpha1
      updateStrategy:
        rollingUpdate:
          maxUnavailable: 1
        type: RollingUpdate
kind: ConfigMap
metadata:
  name: csi-manifests
  namespace: '${NAMESPACE}'
---
apiVersion: v1
kind: Secret
metadata:
  name: cloud-provider-vsphere-credentials
  namespace: '${NAMESPACE}'
stringData:
  data: |-
    apiVersion: v1
    kind: Secret
    metadata:
      labels:
        component: cloud-controller-manager
        vsphere-cpi-infra: secret
      name: cloud-provider-vsphere-credentials
      namespace: kube-system
    stringData:
      ${VSPHERE_SERVER}.password: "${VSPHERE_PASSWORD}"
      ${VSPHERE_SERVER}.username: "${VSPHERE_USERNAME}"
    type: Opaque
type: addons.cluster.x-k8s.io/resource-set
---
apiVersion: v1
data:
  data: |-
    ---
    # Source: vsphere-cpi/templates/service-account.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: cloud-controller-manager
      labels:
        app: vsphere-cpi
        vsphere-cpi-infra: service-account
        component: cloud-controller-manager
      namespace: kube-system
    ---
    # Source: vsphere-cpi/templates/role.yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: cloud-controller-manager
      labels:
        app: vsphere-cpi
        vsphere-cpi-infra: role
        component: cloud-controller-manager
    rules:
      - apiGroups:
          - ""
        resources:
          - events
        verbs:
          - create
          - patch
          - update
      - apiGroups:
          - ""
        resources:
          - nodes
        verbs:
          - "*"
      - apiGroups:
          - ""
        resources:
          - nodes/status
        verbs:
          - patch
      - apiGroups:
          - ""
        resources:
          - services
        verbs:
          - list
          - patch
          - update
          - watch
      - apiGroups:
          - ""
        resources:
          - services/status
        verbs:
          - patch
      - apiGroups:
          - ""
        resources:
          - serviceaccounts
        verbs:
          - create
          - get
          - list
          - watch
          - update
      - apiGroups:
          - ""
        resources:
          - persistentvolumes
        verbs:
          - get
          - list
          - update
          - watch
      - apiGroups:
          - ""
        resources:
          - endpoints
        verbs:
          - create
          - get
          - list
          - watch
          - update
      - apiGroups:
          - ""
        resources:
          - secrets
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - "coordination.k8s.io"
        resources:
          - leases
        verbs:
          - create
          - get
          - list
          - watch
          - update
    ---
    # Source: vsphere-cpi/templates/daemonset.yaml
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: vsphere-cpi
      labels:
        app: vsphere-cpi
        vsphere-cpi-infra: daemonset
        component: cloud-controller-manager
        tier: control-plane
      namespace: kube-system
      annotations:
    spec:
      selector:
        matchLabels:
          app: vsphere-cpi
      updateStrategy:
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: vsphere-cpi
            component: cloud-controller-manager
            tier: control-plane
            release: release-name
            vsphere-cpi-infra: daemonset
        spec:
          tolerations:
            - effect: NoSchedule
              key: node.cloudprovider.kubernetes.io/uninitialized
              value: "true"
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Exists
            - effect: NoSchedule
              key: node.kubernetes.io/not-ready
              operator: Exists
            - effect: NoExecute
              key: CriticalAddonsOnly
              operator: Exists
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
          securityContext:
            fsGroup: 1001
            runAsUser: 1001
          serviceAccountName: cloud-controller-manager
          hostNetwork: true
          dnsPolicy: ClusterFirst
          priorityClassName: system-node-critical
          containers:
          - name: vsphere-cpi
            image: registry.k8s.io/cloud-pv-vsphere/cloud-provider-vsphere:v1.31.1
            imagePullPolicy: IfNotPresent
            args:
              - --cloud-provider=vsphere
              - --v=2
              - --cloud-config=/etc/cloud/vsphere.conf
            volumeMounts:
              - mountPath: /etc/cloud
                name: vsphere-config-volume
                readOnly: true
          volumes:
            - name: vsphere-config-volume
              configMap:
                name: cloud-config
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      labels:
        app: vsphere-cpi
        component: cloud-controller-manager
        vsphere-cpi-infra: role-binding
      name: servicecatalog.k8s.io:apiserver-authentication-reader
      namespace: kube-system
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: extension-apiserver-authentication-reader
    subjects:
    - apiGroup: ""
      kind: ServiceAccount
      name: cloud-controller-manager
      namespace: kube-system
    - apiGroup: ""
      kind: User
      name: cloud-controller-manager
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      labels:
        app: vsphere-cpi
        component: cloud-controller-manager
        vsphere-cpi-infra: cluster-role-binding
      name: cloud-controller-manager
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cloud-controller-manager
    subjects:
    - kind: ServiceAccount
      name: cloud-controller-manager
      namespace: kube-system
    - kind: User
      name: cloud-controller-manager
    ---
    apiVersion: v1
    data:
      vsphere.conf: |
        global:
          port: 443
          secretName: cloud-provider-vsphere-credentials
          secretNamespace: kube-system
          thumbprint: '${VSPHERE_TLS_THUMBPRINT}'
        vcenter:
          ${VSPHERE_SERVER}:
            datacenters:
            - '${VSPHERE_DATACENTER}'
            server: '${VSPHERE_SERVER}'
    kind: ConfigMap
    metadata:
      name: cloud-config
      namespace: kube-system
kind: ConfigMap
metadata:
  name: cpi-manifests


apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: vsphere-rke2-example
  name: vsphere-rke2-example
spec:
  topology:
    class: vsphere-rke2-example
    version: v1.31.7-rke2r1
    controlPlane:
      replicas: 3
    workers:
      machineDeployments:
      - class: vsphere-rke2-example-worker
        name: md-0
        replicas: 3
    variables:
    - name: credsSecretName
      value: vsphere-rke2-example
    - name: infraServer
      value:
        url: '${VSPHERE_SERVER}'
        thumbprint: '${VSPHERE_TLS_THUMBPRINT}'
    - name: vsphereDataCenter
      value: '${VSPHERE_DATACENTER}'
    - name: vsphereDataStore
      value: '${VSPHERE_DATASTORE}'
    - name: vsphereFolder
      value: '${VSPHERE_FOLDER}'
    - name: vsphereNetwork
      value: '${VSPHERE_NETWORK}'
    - name: vsphereResourcePool
      value: '${VSPHERE_RESOURCE_POOL}'
    - name: vsphereServer
      value: '${VSPHERE_SERVER}'
    - name: vsphereTemplate
      value: '${VSPHERE_TEMPLATE}'
    - name: controlPlaneIpAddr
      value: '${CONTROL_PLANE_ENDPOINT_IP}'
    - name: controlPlanePort
      value: 6443
    - name: kubeVipPodManifest
      value: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kube-vip
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          name: system:kube-vip-role
        rules:
          - apiGroups: [""]
            resources: ["services", "services/status", "nodes"]
            verbs: ["list","get","watch", "update"]
          - apiGroups: ["coordination.k8s.io"]
            resources: ["leases"]
            verbs: ["list", "get", "watch", "update", "create"]
        ---
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
          name: system:kube-vip-binding
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:kube-vip-role
        subjects:
        - kind: ServiceAccount
          name: kube-vip
          namespace: kube-system
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          creationTimestamp: null
          name: kube-vip
          namespace: kube-system
        spec:
          containers:
          - args:
            - manager
            env:
            - name: vip_arp
              value: "true"
            - name: port
              value: "6443"
            - name: vip_nodename
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: vip_interface
              value: eth0
            - name: vip_cidr
              value: "32"
            - name: dns_mode
              value: first
            - name: cp_enable
              value: "true"
            - name: cp_namespace
              value: kube-system
            - name: svc_enable
              value: "true"
            - name: svc_leasename
              value: plndr-svcs-lock
            - name: vip_leaderelection
              value: "true"
            - name: vip_leasename
              value: plndr-cp-lock
            - name: vip_leaseduration
              value: "5"
            - name: vip_renewdeadline
              value: "3"
            - name: vip_retryperiod
              value: "1"
            - name: address
              value: ${CONTROL_PLANE_ENDPOINT_IP}
            - name: prometheus_server
              value: :2112
            image: ghcr.io/kube-vip/kube-vip:v0.8.10
            imagePullPolicy: IfNotPresent
            name: kube-vip
            resources: {}
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
                drop:
                - ALL
            volumeMounts:
            - mountPath: /etc/kubernetes/admin.conf
              name: kubeconfig
          hostAliases:
          - hostnames:
            - kubernetes
            ip: 127.0.0.1
          hostNetwork: true
          serviceAccountName: kube-vip
          volumes:
          - hostPath:
              path: /etc/rancher/rke2/rke2.yaml
              type: File
            name: kubeconfig
----
======
