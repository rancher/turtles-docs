= ClusterClass

In this section we cover using https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/[ClusterClass] with {product_name}.

== Prerequisites

* Rancher Manager cluster with {product_name} installed

== Setup

[tabs]
======

Azure::
+
To prepare the management Cluster, we are going to install the https://capz.sigs.k8s.io/[Cluster API Provider Azure], and create a https://capz.sigs.k8s.io/topics/identities#service-principal[ServicePrincipal] identity to provision a new Cluster on Azure. +
Before we start, a ServicePrincipal needs to be created, with at least Contributor access to an Azure subscription. +
Refer to the https://capz.sigs.k8s.io/getting-started#setting-up-your-azure-environment[CAPZ documentation] for more details. +
+
* Provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capz-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: azure
  namespace: capz-system
spec:
  type: infrastructure
  name: azure
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----
+
* Identity Setup
+
In this example we are going to use an `AzureClusterIdentity` to provision Azure Clusters. +
A Secret containing the Service Principal credentials needs to be created first, to be referenced by the `AzureClusterIdentity` resource. +
Note that the `AzureClusterIdentity` is a namespaced resource and it needs to be created in the same namespace as the Cluster. +
For more information on best practices when using Azure identities, please refer to the official https://capz.sigs.k8s.io/topics/identities-use-cases[documentation]. +
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: <AZURE_CLUSTER_IDENTITY_SECRET_NAME>
  namespace: <AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE>
type: Opaque
stringData:
  clientSecret: <AZURE_CLIENT_SECRET>
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AzureClusterIdentity
metadata:
  labels:
    clusterctl.cluster.x-k8s.io/move-hierarchy: "true"
  name: cluster-identity
spec:
  allowedNamespaces: {}
  clientID: <AZURE_APP_ID>
  clientSecret:
    name: <AZURE_CLUSTER_IDENTITY_SECRET_NAME>
    namespace: <AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE>
  tenantID: <AZURE_TENANT_ID>
  type: ServicePrincipal
----

AWS::
+
To prepare the management Cluster, we are going to install the https://cluster-api-aws.sigs.k8s.io/[Cluster API Provider AWS], and create a secret with the required credentials to provision a new Cluster on AWS. +
The global credentials are set to blanks, as we are going to use `AWSClusterStaticIdentity` instead. 
+
* Provider installation
+
[source,yaml]
----
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: aws
  namespace: capa-system
spec:
  type: infrastructure
  variables:
    AWS_B64ENCODED_CREDENTIALS: ""
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----
+
* Identity Setup
+
In this example we are going to use a `AWSClusterStaticIdentity` to provision AWS Clusters. +
A Secret containing the credentials needs to be created in the namespace where the AWS provider is installed. +
For more information on how to setup the credentials, refer to the link:https://cluster-api-aws.sigs.k8s.io/clusterawsadm/clusterawsadm[clusterawsadm documentation]. +
The `AWSClusterStaticIdentity` can reference this Secret to allow Cluster provisioning. For this example we are allowing usage of the identity across all namespaces, so that it can be easily reused. +
You can refer to the link:https://cluster-api-aws.sigs.k8s.io/topics/multitenancy[official documentation] to learn more about identity management.
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: <AWS_IDENTITY_SECRET_NAME>
  namespace: capa-system
type: Opaque
stringData:
  AccessKeyID: <AWS_ACCESS_KEY_ID>
  SecretAccessKey: <AWS_SECRET_ACCESS_KEY>
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSClusterStaticIdentity
metadata:
  name: cluster-identity
spec:
  secretRef: <AWS_IDENTITY_SECRET_NAME>
  allowedNamespaces:
    selector:
      matchLabels: {}
----

Docker::
+
To prepare the management Cluster, we are going to install the Docker Cluster API Provider.
+
* Infrastructure Docker provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capd-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: docker
  namespace: capd-system
spec:
  type: infrastructure
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----

vSphere::
+
To prepare the management Cluster, we are going to install the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/main/docs/getting_started.md[Cluster API Provider vSphere]. +
The global credentials are set to blanks, as we are going to use `VSphereClusterIdentity` instead.  
+
* Provider installation
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capv-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: vsphere
  namespace: capv-system
spec:
  type: infrastructure
  variables:
    VSPHERE_USERNAME: "" 
    VSPHERE_PASSWORD: ""
----
+
* https://github.com/rancher/cluster-api-provider-rke2[Bootstrap/Control Plane provider for RKE2](installed by default) or https://github.com/kubernetes-sigs/cluster-api[Bootstrap/Control Plane provider for Kubeadm], example of Kubeadm installation:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-bootstrap-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-bootstrap
  namespace: capi-kubeadm-bootstrap-system
spec:
  name: kubeadm
  type: bootstrap
---
apiVersion: v1
kind: Namespace
metadata:
  name: capi-kubeadm-control-plane-system
---
apiVersion: turtles-capi.cattle.io/v1alpha1
kind: CAPIProvider
metadata:
  name: kubeadm-control-plane
  namespace: capi-kubeadm-control-plane-system
spec:
  name: kubeadm
  type: controlPlane
----
+
* Identity Setup
+
In this example we are going to use a `VSphereClusterIdentity` to provision vSphere Clusters. +
A Secret containing the credentials needs to be created in the namespace where the vSphere provider is installed. +
The `VSphereClusterIdentity` can reference this Secret to allow Cluster provisioning. For this example we are allowing usage of the identity across all namespaces, so that it can be easily reused. +
You can refer to the https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/main/docs/identity_management.md[official documentation] to learn more about identity management.
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: cluster-identity
  namespace: capv-system
type: Opaque
stringData:
  username: xxx
  password: xxx
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VSphereClusterIdentity
metadata:
  name: cluster-identity
spec:
  secretName: cluster-identity
  allowedNamespaces:
    selector:
      matchLabels: {}

----
======


== Create a Cluster from a ClusterClass

[WARNING]
====
- Examples using `HelmApps` need at least Rancher `v2.11`, or otherwise Fleet `v0.12` or higher.
- Currently, we only support initial provisioning with 1 control plane replica for Kubeadm providers; this can be later scaled up, https://github.com/rancher/turtles/issues/1402[refer]. +
====

[tabs]
======

Azure RKE2::
+
* An Azure ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r azure-rke2 | kubectl apply -f -
----
+
* Create the Azure Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
Also beware that the `internal-first` `registrationMethod` variable is used as a workaround for correct provisioning. +
This immutable variable however will lead to issues when scaling or rolling out control plane nodes. +
A https://github.com/kubernetes-sigs/cluster-api-provider-azure/pull/5525[patch] will support this case in a future release of CAPZ, but the Cluster will need to be reprovisioned to change the `registrationMethod` +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
    cloud-provider: azure
    cni: calico
  name: azure-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-rke2-example
    controlPlane:
      replicas: 3
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    - name: registrationMethod
      value: internal-first
    version: v1.31.7+rke2r1
    workers:
      machineDeployments:
      - class: rke2-default-worker
        name: md-0
        replicas: 3
----

Azure AKS::
+
* An Azure AKS ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r azure-aks | kubectl apply -f -
----
+
* Create the Azure AKS Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
  name: azure-aks-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-aks-example
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    version: v1.31.4
    workers:
      machinePools:
      - class: default-system
        name: system-1
        replicas: 1
      - class: default-worker
        name: worker-1
        replicas: 1
----
Azure Kubeadm::
+
* An Azure ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r azure-kubeadm | kubectl apply -f -
----
+
* Create the Azure Cluster from the example ClusterClass +
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
    cloud-provider: azure
    cni: calico
  name: azure-kubeadm-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: azure-kubeadm-example
    controlPlane:
      replicas: 1
    variables:
    - name: subscriptionID
      value: <AZURE_SUBSCRIPTION_ID>
    - name: location
      value: <AZURE_LOCATION>
    - name: resourceGroup
      value: <AZURE_RESOURCE_GROUP>
    - name: azureClusterIdentityName
      value: cluster-identity
    version: v1.31.1
    workers:
      machineDeployments:
      - class: kubeadm-default-worker
        name: md-0
        replicas: 1
----

AWS Kubeadm::
+
* An AWS Kubeadm ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r aws-kubeadm | kubectl apply -f -
----
+
* Create the AWS Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster-api.cattle.io/rancher-auto-import: "true"
    cni: calico
    cloud-provider: aws
    csi: aws-ebs-csi-driver
  name: aws-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: aws-kubeadm-example
    controlPlane:
      replicas: 1
    variables:
    - name: region
      value: eu-west-2
    - name: sshKeyName
      value: <AWS_SSH_KEY_NAME>
    - name: controlPlaneMachineType
      value: <AWS_CONTROL_PLANE_MACHINE_TYPE>
    - name: workerMachineType
      value: <AWS_NODE_MACHINE_TYPE>
    - name: awsClusterIdentityName
      value: cluster-identity
    version: v1.31.0
    workers:
      machineDeployments:
      - class: default-worker
        name: md-0
        replicas: 1
----

AWS RKE2::
+
[WARNING]
====
Before creating an AWS+RKE2 workload cluster, it is required to either build an AMI for the RKE2 version that is going to be installed on the cluster or find one that will work for non-airgapped installations. 
You can follow the steps in the https://github.com/rancher/cluster-api-provider-rke2/tree/main/image-builder#aws[RKE2 image-builder README] to build the AMI. 
====
+
* An AWS RKE2 ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r aws-rke2 | kubectl apply -f -
----
+
* Create the AWS Cluster from the example ClusterClass +
+ 
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cloud-provider: aws
    csi: aws-ebs-csi-driver
    cluster-api.cattle.io/rancher-auto-import: "true"
  name: aws-quickstart
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: aws-rke2-example
    controlPlane:
      replicas: 1
    variables:
    - name: cni
      value: calico
    - name: region
      value: <AWS_REGION>
    - name: sshKeyName
      value: <AWS_SSH_KEY_NAME>
    - name: controlPlaneMachineType
      value: <AWS_RKE2_CONTROL_PLANE_MACHINE_TYPE>
    - name: workerMachineType
      value: <AWS_RKE2_NODE_MACHINE_TYPE>
    - name: amiID
      value: <AWS_AMI_ID>
    - name: awsClusterIdentityName
      value: cluster-identity
    version: v1.31.7+rke2r1
    workers:
      machineDeployments:
      - class: default-worker
        name: md-0
        replicas: 1
----

Docker Kubeadm::
+
* A Docker Kubeadm ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r docker-kubeadm | kubectl apply -f -
----
+
* Create the Docker Kubeadm Cluster from the example ClusterClass +
+
Note that some variables are left to the user to substitute. +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: docker-kubeadm-quickstart
  labels:
    cni: calico
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
        - 10.96.0.0/24
  topology:
    class: docker-kubeadm-example
    controlPlane:
      replicas: 3
    version: v1.31.4
    workers:
      machineDeployments:
        - class: default-worker
          name: md-0
          replicas: 3
----

Docker RKE2::
+
* A Docker RKE2 ClusterClass can be found among the https://github.com/rancher/turtles/tree/main/examples/clusterclasses[Turtles examples].
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r docker-rke2 | kubectl apply -f -
----
+
* Create the Docker Kubeadm Cluster from the example ClusterClass +
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster 
metadata:
  name: docker-rke2-example
  labels:
    cni: calico
  annotations:
    cluster-api.cattle.io/upstream-system-agent: "true"
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
    services:
      cidrBlocks:
      - 10.96.0.0/24
    serviceDomain: cluster.local
  topology:
    class: docker-rke2-example
    controlPlane:
      replicas: 3
    variables:
    - name: rke2CNI
      value: none
    - name: dockerImage
      value: kindest/node:v1.31.6
    version: v1.31.7+rke2r1
    workers:
      machineDeployments:
      - class: default-worker
        name: md-0
        replicas: 3
----

vSphere Kubeadm::
+
* A vSphere Kubeadm ClusterClass and associated applications can be applied using the examples tool:
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r vsphere-kubeadm | kubectl apply -f -
----
+
* Cluster configuration
+
The vSphere Cloud Provider and the vSphere CSI controller need additional configuration to be applied on the downstream Cluster. +
Similarly to the steps above, we can create two additional Fleet Bundles, that will be applied to the downstream Cluster. +
Please beware that these Bundles are configured to target the downstream Cluster by name: `vsphere-kubeadm-quickstart`. +
If you use a different name for your Cluster, change the Bundle targets accordingly.  
+
[source,yaml]
----
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: vsphere-csi-config
spec:
  resources:
  - content: |-
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: vsphere-config-secret
        namespace: vmware-system-csi
      stringData:
        csi-vsphere.conf: |+
          [Global]
          thumbprint = "<VSPHERE_THUMBPRINT>"

          [VirtualCenter "<VSPHERE_SERVER>"]
          user = "<VSPHERE_USER>"
          password = "<VSPHERE_PASSWORD>"
          datacenters = "<VSPHERE_DATACENTED>"

          [Network]
          public-network = "<VSPHERE_NETWORK>"

          [Labels]
          zone = ""
          region = ""
  targets:
  - clusterSelector:
      matchLabels:
        csi: vsphere
        cluster.x-k8s.io/cluster-name: 'vsphere-kubeadm-quickstart'
---
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: vsphere-cloud-credentials
spec:
  resources:
  - content: |-
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: vsphere-cloud-secret
        namespace: kube-system
      stringData:
        <VSPHERE_SERVER>.password: "<VSPHERE_PASSWORD>"
        <VSPHERE_SERVER>.username: "<VSPHERE_USER>"
  targets:
  - clusterSelector:
      matchLabels:
        cloud-provider: vsphere
        cluster.x-k8s.io/cluster-name: 'vsphere-kubeadm-quickstart'

----
+
* Create the vSphere Cluster from the example ClusterClass +
+
Note that for this example we are using https://kube-vip.io/[kube-vip] as a Control Plane load balancer. +
The `KUBE_VIP_INTERFACE` will be used to bind the `CONTROL_PLANE_IP` in ARP mode. Depending on your operating system and network device configuration, you need to configure this value accordingly - for example, to `eth0`. +
The `kube-vip` static manifest is embedded in the ClusterClass definition. For more information on how to generate a static kube-vip manifest for your own ClusterClasses, please consult the official https://kube-vip.io/docs/installation/static/[documentation].  
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cni: calico
    cloud-provider: vsphere
    csi: vsphere
    cluster-api.cattle.io/rancher-auto-import: "true"
  name: 'vsphere-kubeadm-quickstart'
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: vsphere-kubeadm-example
    version: v1.31.4
    controlPlane:
      replicas: 1
    workers:
      machineDeployments:
      - class: vsphere-kubeadm-example-worker
        name: md-0
        replicas: 1
    variables:
    - name: vSphereClusterIdentityName
      value: cluster-identity
    - name: vSphereTLSThumbprint
      value: <VSPHERE_THUMBPRINT>
    - name: vSphereDataCenter
      value: <VSPHERE_DATACENTER>
    - name: vSphereDataStore
      value: <VSPHERE_DATASTORE>
    - name: vSphereFolder
      value: <VSPHERE_FOLDER>
    - name: vSphereNetwork
      value: <VSPHERE_NETWORK>
    - name: vSphereResourcePool
      value: <VSPHERE_RESOURCE_POOL>
    - name: vSphereServer
      value: <VSPHERE_SERVER>
    - name: vSphereTemplate
      value: <VSPHERE_TEMPLATE>
    - name: controlPlaneIpAddr
      value: <CONTROL_PLANE_IP>
    - name: controlPlanePort
      value: 6443
    - name: sshKey
      value: <SSH_KEY>
    - name: kubeVIPInterface
      value: <KUBE_VIP_INTERFACE>
----

vSphere RKE2::
+
* A vSphere RKE2 ClusterClass and associated applications can be applied using the examples tool:
+
[source,bash]
----
go run github.com/rancher/turtles/examples@latest -r vsphere-rke2 | kubectl apply -f -
----
+
* Cluster configuration
+
The vSphere Cloud Provider and the vSphere CSI controller need additional configuration to be applied on the downstream Cluster. +
Similarly to the steps above, we can create two additional Fleet Bundles, that will be applied to the downstream Cluster. +
Please beware that these Bundles are configured to target the downstream Cluster by name: `vsphere-rke2-quickstart`. +
If you use a different name for your Cluster, change the Bundle targets accordingly.  
+
[source,yaml]
----
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: vsphere-csi-config
spec:
  resources:
  - content: |-
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: vsphere-config-secret
        namespace: vmware-system-csi
      stringData:
        csi-vsphere.conf: |+
          [Global]
          thumbprint = "<VSPHERE_THUMBPRINT>"

          [VirtualCenter "<VSPHERE_SERVER>"]
          user = "<VSPHERE_USER>"
          password = "<VSPHERE_PASSWORD>"
          datacenters = "<VSPHERE_DATACENTED>"

          [Network]
          public-network = "<VSPHERE_NETWORK>"

          [Labels]
          zone = ""
          region = ""
  targets:
  - clusterSelector:
      matchLabels:
        csi: vsphere
        cluster.x-k8s.io/cluster-name: 'vsphere-rke2-quickstart'
---
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: vsphere-cloud-credentials
spec:
  resources:
  - content: |-
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: vsphere-cloud-secret
        namespace: kube-system
      stringData:
        <VSPHERE_SERVER>.password: "<VSPHERE_PASSWORD>"
        <VSPHERE_SERVER>.username: "<VSPHERE_USER>"
  targets:
  - clusterSelector:
      matchLabels:
        cloud-provider: vsphere
        cluster.x-k8s.io/cluster-name: 'vsphere-rke2-quickstart'

----
+
* Create the vSphere Cluster from the example ClusterClass +
+
Note that for this example we are using https://kube-vip.io/[kube-vip] as a Control Plane load balancer. +
The `KUBE_VIP_INTERFACE` will be used to bind the `CONTROL_PLANE_IP` in ARP mode. Depending on your operating system and network device configuration, you need to configure this value accordingly - for example, to `eth0`. +
The `kube-vip` static manifest is embedded in the ClusterClass definition. For more information on how to generate a static kube-vip manifest for your own ClusterClasses, please consult the official https://kube-vip.io/docs/installation/static/[documentation]. +
In case you are using a VM template based on SUSE Linux Micro, you may optionally provide a `productKey` variable to enable automatic SL Micro registration against SUSE Customer Center.
+
[source,yaml]
----
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cni: calico
    cloud-provider: vsphere
    csi: vsphere
    cluster-api.cattle.io/rancher-auto-import: "true"
  name: 'vsphere-rke2-quickstart'
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  topology:
    class: vsphere-rke2-example
    version: v1.31.7+rke2r1
    controlPlane:
      replicas: 1
    workers:
      machineDeployments:
      - class: vsphere-rke2-example-worker
        name: md-0
        replicas: 1
    variables:
    - name: vSphereClusterIdentityName
      value: cluster-identity
    - name: vSphereTLSThumbprint
      value: <VSPHERE_THUMBPRINT>
    - name: vSphereDataCenter
      value: <VSPHERE_DATACENTER>
    - name: vSphereDataStore
      value: <VSPHERE_DATASTORE>
    - name: vSphereFolder
      value: <VSPHERE_FOLDER>
    - name: vSphereNetwork
      value: <VSPHERE_NETWORK>
    - name: vSphereResourcePool
      value: <VSPHERE_RESOURCE_POOL>
    - name: vSphereServer
      value: <VSPHERE_SERVER>
    - name: vSphereTemplate
      value: <VSPHERE_TEMPLATE>
    - name: controlPlaneIpAddr
      value: <CONTROL_PLANE_IP>
    - name: controlPlanePort
      value: 6443
    - name: sshKey
      value: <SSH_KEY>
    - name: kubeVIPInterface
      value: <KUBE_VIP_INTERFACE>
    - name: productKey
      value: <SL_MICRO_PRODUCT_KEY>
----
======


== Optionally Mark Namespace for Auto-Import

To automatically import a CAPI cluster into Rancher Manager, you can label a namespace so all clusters contained in it are imported.

[source,bash]
----
export NAMESPACE=default
kubectl label namespace $NAMESPACE cluster-api.cattle.io/rancher-auto-import=true
----
